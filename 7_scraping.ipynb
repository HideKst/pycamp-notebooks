{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Scraping\"></a>\n",
    "# 7. スクレイピング\n",
    "***\n",
    "### 自動でデータを収集する方法\n",
    "\n",
    "## 7.1. スクレイピングとは\n",
    "***\n",
    "ウェブサイトから情報を抽出する、コンピュータソフトウェア技術のことをいいます。\n",
    "\n",
    "ここではPythonを使った実用的なプログラムの例として、スクレイピングを行います。\n",
    "\n",
    "\n",
    "## 7.2. 環境構築\n",
    "***\n",
    "前章の「 [6.2 venvとは](http://localhost:8888/notebooks/7_scraping.ipynb#about-venv)」を参考に、venvモジュールを利用して、スクレイピング用のvenv環境を構築します。\n",
    "venv環境を ``activate`` コマンドで有効にし、スクレイピングに使用するRequestsとBeautiful Soup 4を ``pip`` コマンドでインストールします。\n",
    "#### リスト7.1 スクレイピング用のvenv環境を構築\n",
    "```bash\n",
    "$ mkdir scraping\n",
    "$ cd scraping\n",
    "$ python3 -m venv env  # Windowsの場合は python -m venv env\n",
    "$ source env/bin/activate\n",
    "(env) $ pip install requests\n",
    "(env) $ pip install beautifulsoup4\n",
    "```\n",
    "<a id=\"Requests\"></a>\n",
    "### 7.2.1. Requests\n",
    "***\n",
    ":URL: http://docs.python-requests.org/en/master/\n",
    "\n",
    "Requests について簡単に紹介します。\n",
    "Requests はウェブサイトにアクセスしてHTMLなどのデータを取得するためのライブラリです。\n",
    "Pythonの標準ライブラリ [urllib.request](https://docs.python.org/ja/3/library/urllib.request.html) でも同様のことは行なえますが、より便利な Requests をここでは使用します。\n",
    "\n",
    "<a id=\"Beautiful-Soup-4\"></a>\n",
    "### 7.2.2. Beautiful Soup 4\n",
    "***\n",
    ":URL: https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "\n",
    "Beautiful Soup 4はHTMLやXMLの中身を解析して、任意の情報を取得するためのライブラリです。\n",
    "Pythonの標準ライブラリ [html.parser](https://docs.python.org/ja/3/library/html.parser.html) でも同様のことは行なえますが、より便利な Beautiful Soup 4 をここでは使用します。\n",
    "なお、beautifulsoupとbeautifulsoup4が存在しますが、新しい **beautifulsoup4** を使うようにしてください。\n",
    "\n",
    "## 7.3. シンプルなスクレイピングのコード\n",
    "***\n",
    "スクレイピングの例として、PyCon JP 2017のスポンサー一覧のページ(https://pycon.jp/2017/ja/sponsors/)からスポンサー名とURLの情報を抜き出します。\n",
    "\n",
    "![スポンサー一覧ページ](images/sponsor-list.png)\n",
    "<center>スポンサー一覧ページ</center>\n",
    "\n",
    "下記コードをsimple.pyという名前で保存します(:numref:`simple-py`)。\n",
    "\n",
    "<a id=\"simple-py\"></a>\n",
    "#### リスト7.2 simple.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def main():\n",
    "        url = 'https://pycon.jp/2017/ja/sponsors/'\n",
    "        res = requests.get(url)\n",
    "        content = res.content\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        sponsors = soup.find_all('div', class_='sponsor-content')\n",
    "        for sponsor in sponsors:\n",
    "               url = sponsor.h3.a['href']\n",
    "               name = sponsor.h4.text\n",
    "               print(name, url)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "       main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このコードを実行すると、以下のようにスポンサー名とURLの一覧が取得できます([リスト7.3](#exec-simple-py))。\n",
    "\n",
    "<a id=\"exec-simple-py\"></a>\n",
    "#### リスト7.3 スクレイピングを実行\n",
    "```bash\n",
    "(env) $ python simple.py\n",
    "株式会社SQUEEZE https://squeeze-inc.co.jp/\n",
    "株式会社MonotaRO https://recruit.monotaro.com/?utm_medium=outside_flier&utm_source=pycon.jp&utm_campaign=PyConJP2017\n",
    "LINE株式会社 https://engineering.linecorp.com/\n",
    "Retty株式会社 http://corp.retty.me/\n",
    "iRidge, Inc. https://iridge.jp/\n",
    "株式会社いい生活 http://www.e-seikatsu.info/recruit/graduate/\n",
    ":\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"PEP8\"></a>\n",
    "\n",
    "### コラム: Pythonのコーディング規約「PEP8」\n",
    "\n",
    "Pythonには [PEP8（ペップエイト）](https://www.python.org/dev/peps/pep-0008/) というコーディング規約があります。\n",
    "チームで開発をする際、人によってプログラムコードの書き方がバラバラだと読みにくいコードになってしまいます。\n",
    "そのため、PEP8のルールに従う習慣を身につけておくとよいでしょう。\n",
    "\n",
    "コードがPEP8のルールに従っているかは、 [pycodestyle](http://pep8.readthedocs.io/en/latest/index.html#) というツールで検証できます(以前はツールの名前もpep8でした)。\n",
    "\n",
    "pycodestyleは ``pip install pycodestyle`` でインストールして使用します。\n",
    "``simple.py`` を検証するには、 ``pycodestyle simple.py`` を実行します。\n",
    "\n",
    "\n",
    "### 7.3.1. コードの解説\n",
    "***\n",
    "上記のコードがどういった内容なのかを解説します。\n",
    "\n",
    "* 以下のコードはRequestsとBeautiful Soup 4をimportして利用できるようにしています。\n",
    "#### リスト7.4 モジュールのimport\n",
    "```python\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "```\n",
    "* メインとなる処理を ``main`` 関数として定義しています。\n",
    "  なお、関数の名前に特に決まりはなく、必ずしも ``main`` である必要はありません。\n",
    "\n",
    "#### リスト7.5 main()関数の定義\n",
    "```python\n",
    "def main():\n",
    "```\n",
    "* Requestsを使用して、Webページの内容(HTML)を取得します。res.contentにHTMLの中身が文字列データとして入っています。\n",
    "\n",
    "#### リスト7.6 ページの内容を取得\n",
    "```python\n",
    "url = 'https://pycon.jp/2017/ja/sponsors/'\n",
    "res = requests.get(url)\n",
    "content = res.content\n",
    "```\n",
    "* 次にHTMLをBeautiful Soup 4に渡して解析します。HTMLの解析についてはもう少し詳しく説明します。\n",
    "\n",
    "#### リスト7.7 WebページをBeautiful Soup 4で解析\n",
    "```python\n",
    "soup = BeautifulSoup(content, 'html.parser')\n",
    "sponsors = soup.find_all('div', class_='sponsor-content')\n",
    "for sponsor in sponsors:\n",
    "       url = sponsor.h3.a['href']\n",
    "       name = sponsor.h4.text\n",
    "       print(name, url)\n",
    "```\n",
    "\n",
    "* 最後に、このスクリプトが実行された時に、main()関数を実行するように指定します。\n",
    "\n",
    "#### リスト7.8 main()関数を実行\n",
    "```python\n",
    "if __name__ == '__main__':\n",
    "       main()\n",
    "```\n",
    "### 7.3.2. HTMLの解析の解説\n",
    "***\n",
    "Beautiful Soup 4でHTMLを解析して、値が取り出せましたが、どのように指定しているのでしょうか?\n",
    "スポンサー一覧のHTMLを見てみると、以下のような形式になっています。([リスト7.9](#sponsor-list-html)\n",
    "\n",
    "<a id=\"sponsor-list-html\"></a>\n",
    "#### リスト7.9 スポンサー一覧のHTML\n",
    "```html\n",
    "   <div class=\"span12\">\n",
    "     <h2>Diamond</h2>\n",
    "     <div class=\"row\">\n",
    "       <div class=\"span4\">\n",
    "         <div class=\"sponsor\" id=\"sponsor-5\">\n",
    "           <div class=\"sponsor-content\">\n",
    "             <h3>\n",
    "               <a href=\"https://squeeze-inc.co.jp/\">\n",
    "                 <img src=\"/2017/site_media/media/sponsor_files/squeeze-logo-horizontal_1.png.150x80_q85.png\" alt=\"株式会社SQUEEZE\" />\n",
    "               </a>\n",
    "             </h3>\n",
    "             <h4>株式会社SQUEEZE</h4>\n",
    "             <p><a href=\"https://squeeze-inc.co.jp/\">https://squeeze-inc.co.jp/</a></p>\n",
    "             <p>\n",
    "               <p>株式会社SQUEEZEでは「価値の詰まった社会を創る」ことをミッションとしております。ICTの力で地域コミュニティが持つ資産の潜在的な「価値」を活かし、社会に提供していくことで「無駄」のない「価値の詰まった」社会を創造していきます。</p>\n",
    "               <p>主要事業として、ホテル・旅館・民泊に特化したサービスを提供・運営しています。ホスピタリティテックのリーディングカンパニーとして、人材の再発掘・活用による働き方改革、空き家問題解消による地域活性化、を牽引するナンバーワンのプラットフォームになることを目指しています。</p>\n",
    "             </p>\n",
    "           </div>\n",
    "         </div>\n",
    "       </div>\n",
    "     </div>\n",
    "   </div>\n",
    "   <div class=\"span12\">\n",
    "     <h2>Platinum</h2>\n",
    "       <div class=\"row\">\n",
    "         <div class=\"span4\">\n",
    "           <div class=\"sponsor\" id=\"sponsor-7\">\n",
    "             <div class=\"sponsor-content\">\n",
    "               <h3>\n",
    "                 <a href=\"https://recruit.monotaro.com/?utm_medium=outside_flier&amp;utm_source=pycon.jp&amp;utm_campaign=PyConJP2017\">\n",
    "                   <img src=\"/2017/site_media/media/sponsor_files/logo-PyCon2017.png.150x80_q85.png\" alt=\"株式会社MonotaRO\" />\n",
    "                 </a>\n",
    "               </h3>\n",
    "               <h4>株式会社MonotaRO</h4>\n",
    "               <p><a href=\"https://recruit.monotaro.com/?utm_medium=outside_flier&amp;utm_source=pycon.jp&amp;utm_campaign=PyConJP2017\">https://recruit.monotaro.com/?utm_medium=outside_flier&amp;utm_source=pycon.jp&amp;utm_campaign=PyConJP2017</a></p>\n",
    "               <p>\n",
    "                 <p>「ITで、間接資材調達を変革する」<br />モノタロウは働く現場で必要となる様々な間接資材(最終製品となる原材料を除く全ての資材)約1,000万点をインターネットで販売しています。<br />様々な現場のニーズにお応えすべく、自社開発の高度な検索システムと精緻なデータベースマーケティングが実現する「お客様ごとの最適化したレコメンドサービス」で従来の非効率的な間接資材調達を変革し社会に新しい価値を提供しています。</p>\n",
    "               </p>\n",
    "             </div>\n",
    "   (以下続く)\n",
    "```\n",
    "\n",
    "このHTMLを見ると、スポンサーの名前とURLは以下のようにして取得できそうです。\n",
    "\n",
    "* 一つのスポンサーの情報は ``<div class=\"sponsor-content\">`` の中に入っている\n",
    "* スポンサーのURLは ``<h3>`` タグの中の ``<a>`` タグの ``href`` アトリビュートに入っている\n",
    "* スポンサー名は ``<h4>`` タグで囲まれた中に入っている\n",
    "\n",
    "HTMLの構造がわかったところで、もう一度HTMLを解析しているコードを見てみます。\n",
    "\n",
    "<a id=\"html.parser\"></a>\n",
    "#### リスト7.10 WebページをBeautiful Soup 4で解析\n",
    "```python\n",
    "soup = BeautifulSoup(content, 'html.parser')\n",
    "sponsors = soup.find_all('div', class_='sponsor-content')\n",
    "for sponsor in sponsors:\n",
    "    url = sponsor.h3.a['href']\n",
    "    name = sponsor.h4.text\n",
    "    print(name, url)\n",
    "```\n",
    "まず、 ``soup.find_all()`` メソッドで、全スポンサーの情報が含まれている div 要素を取得しています。\n",
    "次に、各スポンサー情報(sponsor変数に入っている)から値を取り出しています。\n",
    "最初にURLを取得して、次にスポンサー名を取得しています。\n",
    "\n",
    "## 7.4. 作り変えてみよう\n",
    "***\n",
    "RequestsやBeautiful Soup 4の動作を変えて、さまざまなWebページから色んな要素を取得できます。\n",
    "\n",
    "以下にそれぞれのライブラリの簡単な使い方を紹介します。それ以外にもいろいろな使用方法があるので、ドキュメントを参考にしていろいろ作り変えてみてください。\n",
    "\n",
    "<a id=\"Requests\"></a>\n",
    "### 7.4.1 Requests の主な使い方\n",
    "***\n",
    "ここでは Requests の主な使い方の例をいくつか載せます。\n",
    "詳細については以下の公式ドキュメントを参照してください。\n",
    "\n",
    "公式ドキュメント: [Requests: HTTP for Humans](http://docs.python-requests.org/en/master/)\n",
    "\n",
    "以下は認証つきのURLにアクセスして、結果を取得する例です。\n",
    "#### リスト7.11 認証付きURLにアクセスする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "r = requests.get('https://api.github.com/user', auth=('user', 'pass'))\n",
    "r.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Requests-POST\"></a>\n",
    "\n",
    "POST を行う場合は以下のように、POSTのパラメーターを辞書で定義します。\n",
    "\n",
    "#### リスト7.12 requests で POST する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {'key1': 'value1', 'key2': 'value2'} # POST するパラメーター\n",
    "r = requests.post('http://httpbin.org/post', data=payload)\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Requests-GET\"></a>\n",
    "\n",
    "GET に ``?key1=value1&key2=value2`` のようなパラメーター付きでアクセスする場合も同様に、辞書で定義します。\n",
    "\n",
    "#### リスト7.13 requests でパラメーター付で GET する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {'key1': 'value1', 'key2': 'value2'}\n",
    "r = requests.get('http://httpbin.org/get', params=payload)\n",
    "print(r.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {'key1': 'value1', 'key2': ['value2', 'value3']}\n",
    "r = requests.get('http://httpbin.org/get', params=payload)\n",
    "print(r.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.2 Beautiful Soup 4の主な使い方\n",
    "***\n",
    "ここではBeautiful Soup 4の主な使い方の例をいくつか載せます。\n",
    "詳細については以下の公式ドキュメントを参照してください。\n",
    "\n",
    "<a id=\"Beautiful-Soup-4-Documentation\"></a>\n",
    "\n",
    "公式ドキュメント: [Beautiful Soup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "#### リスト7.14  Beautiful Soup 4の使用例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "r = requests.get('https://www.python.org/blogs/')\n",
    "soup = BeautifulSoup(r.content, 'html.parser') # 取得したHTMLを解析\n",
    "soup.title # titleタグの情報を取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title.string # titleタグの文字列を取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(soup.find_all('a')) # 全ての a タグを取得しt len() で件数を取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.python.org/news/'\n",
    "res = requests.get(url)\n",
    "soup = BeautifulSoup(res.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".. index:: find/find_all\n",
    "    single: Beautiful Soup 4; find()\n",
    "    single: Beautiful Soup 4; find_all()\n",
    "\n",
    "また、 ``find()`` ``find_all()`` などでタグを探す場合には、タグの属性などを条件として指定できます。\n",
    "#### リスト7.15 find/find_all の使用例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(soup.find_all('h1')) # 指定したタグを検索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(soup.find_all(['h1', 'h2', 'h3'])) # 複数のタグのいずれかにマッチ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(soup.find_all('h3', {'class': 'event-title'})) # <h3 class=\"event-title\"> にマッチ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5. まとめ\n",
    "***\n",
    "本節では、Pythonでスクレイピングをする方法を解説しました。\n",
    "\n",
    "RequestsとBeautiful Soup 4を使いこなすことにより、さまざまなウェブサイトから情報を取得できるようになります。\n",
    "\n",
    "なお、短時間にWebサイトに大量にアクセスをすると迷惑となるので、そういうことがないようにプログラムを実行するときには注意してください。\n",
    "\n",
    "## 7.6. 参考書籍\n",
    "***\n",
    "Pythonでのスクレイピングについてもいくつか書籍が出ています。\n",
    "\n",
    "- [PythonによるWebスクレイピング](https://www.oreilly.co.jp/books/9784873117614/)\n",
    "- [Pythonクローリング＆スクレイピング ―データ収集・解析のための実践開発ガイド](http://gihyo.jp/book/2017/978-4-7741-8367-1)\n",
    "- [Pythonによるスクレイピング＆機械学習 開発テクニックBeautifulSoup、scikit-learn、TensorFlowを使ってみよう](http://www.socym.co.jp/book/1079)\n",
    "- [Pythonエンジニア ファーストブック](http://gihyo.jp/book/2017/978-4-7741-9222-2) (第4章 PythonによるWebスクレイピング)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
